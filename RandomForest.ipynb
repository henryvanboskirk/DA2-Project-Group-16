{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "training_df = pd.read_csv('train.csv')\n",
    "testing_df = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for X4_mean: 0.16192624201910594\n",
      "RMSE for X11_mean: 15177.763409490393\n",
      "RMSE for X18_mean: 3191701.3825716935\n",
      "RMSE for X26_mean: 323695.84244395595\n",
      "RMSE for X50_mean: 1565.3173075890318\n",
      "RMSE for X3112_mean: 204648663.06719327\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "# Assuming the first column is 'id' and the following columns are the features,\n",
    "# and the last 12 columns are the 6 traits mean and sd\n",
    "# Features selection: Exclude the 'id' column and the target columns (mean and sd values of the traits)\n",
    "X = training_df.drop(columns=['id', 'X4_mean', 'X11_mean', 'X18_mean', 'X26_mean', 'X50_mean', 'X3112_mean', 'X4_sd', 'X11_sd', 'X18_sd', 'X26_sd', 'X50_sd', 'X3112_sd'])\n",
    "\n",
    "# Targets selection: Only the mean values of the traits\n",
    "y_means = training_df[['X4_mean', 'X11_mean', 'X18_mean', 'X26_mean', 'X50_mean', 'X3112_mean']]\n",
    "\n",
    "\n",
    "# Splitting the data into training and validation sets for each trait\n",
    "for column in y_means.columns:\n",
    "    y = y_means[column]\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Initialize the RandomForestRegressor\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on the validation set\n",
    "    predictions = model.predict(X_val)\n",
    "\n",
    "    # Evaluate the model\n",
    "    mse = mean_squared_error(y_val, predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    print(f\"RMSE for {column}: {rmse}\")\n",
    "\n",
    "# Adjust the code for your actual file paths and column indices as necessary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The code above ran for 4 and a half hours on my computer, but it rewrote the models over each other, so we cant use it to make predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 11\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Assuming your models are stored in a dictionary models with keys as trait names\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# e.g., models = {'X4_mean': model_X4, ...}\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m trait \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX4_mean\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX11_mean\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX18_mean\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX26_mean\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX50_mean\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX3112_mean\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# Predict using each model and store the predictions\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m     predictions[trait] \u001b[38;5;241m=\u001b[39m model[trait]\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Create the submission DataFrame\u001b[39;00m\n\u001b[0;32m     14\u001b[0m submission_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m: testing_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpredictions  \u001b[38;5;66;03m# This unpacks the predictions dict into the DataFrame\u001b[39;00m\n\u001b[0;32m     17\u001b[0m })\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_base.py:204\u001b[0m, in \u001b[0;36mBaseEnsemble.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[0;32m    203\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the index'th estimator in the ensemble.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_[index]\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "\n",
    "# Prepare the features from the test set, matching the training feature selection\n",
    "X_test = testing_df.drop(columns=['id'])\n",
    "\n",
    "# Initialize a dictionary to store predictions\n",
    "predictions = {}\n",
    "\n",
    "# Assuming your models are stored in a dictionary models with keys as trait names\n",
    "# e.g., models = {'X4_mean': model_X4, ...}\n",
    "for trait in ['X4_mean', 'X11_mean', 'X18_mean', 'X26_mean', 'X50_mean', 'X3112_mean']:\n",
    "    # Predict using each model and store the predictions\n",
    "    predictions[trait] = model[trait].predict(X_test)\n",
    "\n",
    "# Create the submission DataFrame\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': testing_df['id'],\n",
    "    **predictions  # This unpacks the predictions dict into the DataFrame\n",
    "})\n",
    "\n",
    "# Ensure the column order matches the sample_submission.csv, adjust if necessary\n",
    "submission_df = submission_df[['id', 'X4_mean', 'X11_mean', 'X18_mean', 'X26_mean', 'X50_mean', 'X3112_mean']]\n",
    "\n",
    "# Save the submission file\n",
    "submission_df.to_csv('Random Forest Attempt 1', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'model_X4.joblib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m dump(model, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcolumn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.joblib\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjoblib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load\n\u001b[1;32m----> 7\u001b[0m model_X4 \u001b[38;5;241m=\u001b[39m load(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_X4.joblib\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\numpy_pickle.py:650\u001b[0m, in \u001b[0;36mload\u001b[1;34m(filename, mmap_mode)\u001b[0m\n\u001b[0;32m    648\u001b[0m         obj \u001b[38;5;241m=\u001b[39m _unpickle(fobj)\n\u001b[0;32m    649\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 650\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    651\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m _read_fileobject(f, filename, mmap_mode) \u001b[38;5;28;01mas\u001b[39;00m fobj:\n\u001b[0;32m    652\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fobj, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    653\u001b[0m                 \u001b[38;5;66;03m# if the returned file object is a string, this means we\u001b[39;00m\n\u001b[0;32m    654\u001b[0m                 \u001b[38;5;66;03m# try to load a pickle file generated with an version of\u001b[39;00m\n\u001b[0;32m    655\u001b[0m                 \u001b[38;5;66;03m# Joblib so we load it with joblib compatibility function.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'model_X4.joblib'"
     ]
    }
   ],
   "source": [
    "from joblib import dump\n",
    "\n",
    "# After each model is trained within the loop:\n",
    "dump(model, f'model_{column}.joblib')\n",
    "from joblib import load\n",
    "\n",
    "model_X4 = load('model_X4.joblib')\n",
    "# Repeat for other models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
